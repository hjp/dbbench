#!/usr/bin/python3

"""
Compare 3 ways of importing of a largish csv file into a pg table:
1) Plain inserts
2) Prepare/Execute
3) Copy

Additionally, "import" the data from a different table in the same
database.

We perform each variant 5 times and report the median.
"""

import csv
import os
import psycopg2
import sys
import time
import multiprocessing as mp

db = psycopg2.connect("")
csr = db.cursor()

def create_table():
    with open(sys.argv[1], encoding="utf-8-sig") as f:
        rdr = csv.reader(f,delimiter=",")
        header = next(rdr)
        try:
            create_table.count += 1
        except:
            create_table.count = 1
        table_name = "import_pg_comparison_%d_%d" % (os.getpid(), create_table.count)
        q_drop = "drop table if exists " + table_name
        csr.execute(q_drop);

        q_create = (
            "create table " + table_name + "(" +
            ", ".join('"%s" varchar' % x for x in header) +
            ")"
        )
        print(q_create)
        csr.execute(q_create);
    return table_name

def import_plain():
    table_name = create_table()

    with open(sys.argv[1], encoding="utf-8-sig") as f:
        rdr = csv.reader(f,delimiter=",")
        header = next(rdr)
        q_insert = (
            "insert into " + table_name +
            " values(" + ", ".join(["%s"] * len(header)) + ")"
        )
        print(q_insert)
        db.commit()
        t0 = time.monotonic()
        for line in rdr:
            csr.execute(q_insert, line)

    db.commit()
    t1 = time.monotonic()
    dt = t1 - t0
    csr.execute("select count(*) from " + table_name)
    r = csr.fetchone()[0]
    print("imported %d rows in %f seconds (%f rows per second)" % (r, dt, r / dt))
    q_drop = "drop table if exists " + table_name
    csr.execute(q_drop);
    return r / dt

def import_prepared():
    table_name = create_table()

    with open(sys.argv[1], encoding="utf-8-sig") as f:
        rdr = csv.reader(f,delimiter=",")
        header = next(rdr)
        q_prepare = (
            "prepare ins1 (" + ", ".join(["varchar"] * len(header)) + ") as " +
            "insert into " + table_name +
            " values(" + ", ".join("$%d" % (x + 1) for x in range(0, len(header))) + ")"
        )
        print(q_prepare)
        csr.execute(q_prepare)

        q_exec = (
            "execute ins1(" + ", ".join(["%s"] * len(header)) + ")"
        )
        print(q_exec)

        db.commit()
        t0 = time.monotonic()
        for line in rdr:
            csr.execute(q_exec, line)

    db.commit()
    t1 = time.monotonic()
    dt = t1 - t0
    csr.execute("select count(*) from " + table_name)
    r = csr.fetchone()[0]
    print("imported %d rows in %f seconds (%f rows per second)" % (r, dt, r / dt))
    q_drop = "drop table if exists " + table_name
    csr.execute(q_drop);
    csr.execute("deallocate ins1")
    return r / dt

def import_copy():
    table_name = create_table()

    db.commit()
    t0 = time.monotonic()
    # let's do this in a bit of a roundabout way: Recode the input file
    # to use tab separators, then use copy on that (because psycopg2
    # doesn't like quotes and in reality the input format often isn't
    # quite what copy expects either).
    with open(sys.argv[1], encoding="utf-8-sig") as f:
        rdr = csv.reader(f,delimiter=",")
        header = next(rdr)
        with open("%s.tsv" % table_name, "w", encoding="utf-8") as wf:
            for line in rdr:
                print(*line, sep="\t", file=wf)
    with open("%s.tsv" % table_name, encoding="utf-8") as f:
        csr.copy_from(f, table_name)
    db.commit()
    t1 = time.monotonic()
    dt = t1 - t0
    csr.execute("select count(*) from " + table_name)
    r = csr.fetchone()[0]
    print("imported %d rows in %f seconds (%f rows per second)" % (r, dt, r / dt))
    q_drop = "drop table if exists " + table_name
    csr.execute(q_drop);
    return r / dt

def import_select():
    """
    "Import" data from another table in the database.
    """
    table_name_1 = create_table()
    table_name_2 = create_table()

    db.commit()
    # Initialize table 1 with copy.
    # This isn't part of the actual test.
    with open(sys.argv[1], encoding="utf-8-sig") as f:
        rdr = csv.reader(f,delimiter=",")
        header = next(rdr)
        with open("%s.tsv" % table_name_1, "w", encoding="utf-8") as wf:
            for line in rdr:
                print(*line, sep="\t", file=wf)
    with open("%s.tsv" % table_name_1, encoding="utf-8") as f:
        csr.copy_from(f, table_name_1)
    db.commit()
    t0 = time.monotonic()
    csr.execute("insert into %s select * from %s" % (table_name_2, table_name_1))
    db.commit()
    t1 = time.monotonic()
    dt = t1 - t0
    csr.execute("select count(*) from " + table_name_2)
    r = csr.fetchone()[0]
    print("imported %d rows in %f seconds (%f rows per second)" % (r, dt, r / dt))
    q_drop = "drop table if exists " + table_name_1
    csr.execute(q_drop);
    q_drop = "drop table if exists " + table_name_2
    csr.execute(q_drop);
    db.commit()
    return r / dt

def import_plain_parallel_worker(i, n, file_name, table_name):
    db = psycopg2.connect("")
    csr = db.cursor()

    with open(file_name, encoding="utf-8-sig") as f:
        rdr = csv.reader(f,delimiter=",")
        header = next(rdr)
        q_insert = (
            "insert into " + table_name +
            " values(" + ", ".join(["%s"] * len(header)) + ")"
        )
        print(q_insert)
        c = 0
        for line in rdr:
            if c % n == i:
                csr.execute(q_insert, line)
            c += 1

    db.commit()

def import_plain_parallel(n):
    table_name = create_table()
    db.commit()

    t0 = time.monotonic()
    kids = []
    for i in range(n):
        p = mp.Process(
                target=import_plain_parallel_worker,
                args=(i, n, sys.argv[1], table_name)
               )
        p.start()
        kids.append(p)
    for p in kids:
        p.join()

    t1 = time.monotonic()
    dt = t1 - t0
    csr.execute("select count(*) from " + table_name)
    r = csr.fetchone()[0]
    print("imported %d rows in %f seconds (%f rows per second)" % (r, dt, r / dt))
    q_drop = "drop table if exists " + table_name
    csr.execute(q_drop);
    return r / dt

def import_copy_parallel_worker(i, n, file_name, table_name):
    db = psycopg2.connect("")
    csr = db.cursor()

    with open(file_name, encoding="utf-8-sig") as f:
        rdr = csv.reader(f,delimiter=",")
        header = next(rdr)
        with open("%s_%d.tsv" % (table_name, i), "w", encoding="utf-8") as wf:
            c = 0
            for line in rdr:
                if c % n == i:
                    print(*line, sep="\t", file=wf)
                c += 1
    with open("%s_%d.tsv" % (table_name, i), encoding="utf-8") as f:
        csr.copy_from(f, table_name)
    db.commit()

def import_copy_parallel(n):
    table_name = create_table()
    db.commit()

    t0 = time.monotonic()
    kids = []
    for i in range(n):
        p = mp.Process(
                target=import_copy_parallel_worker,
                args=(i, n, sys.argv[1], table_name)
               )
        p.start()
        kids.append(p)
    for p in kids:
        p.join()

    t1 = time.monotonic()
    dt = t1 - t0
    csr.execute("select count(*) from " + table_name)
    r = csr.fetchone()[0]
    print("imported %d rows in %f seconds (%f rows per second)" % (r, dt, r / dt))
    q_drop = "drop table if exists " + table_name
    csr.execute(q_drop);
    return r / dt



tests = (
    ("import_copy",             import_copy),
    ("import_copy_parallel 2",  lambda: import_copy_parallel(2)),
    ("import_copy_parallel 4",  lambda: import_copy_parallel(4)),
    ("import_copy_parallel 8",  lambda: import_copy_parallel(8)),
    ("import_plain",            import_plain),
    ("import_plain_parallel 2", lambda: import_plain_parallel(2)),
    ("import_plain_parallel 4", lambda: import_plain_parallel(4)),
    ("import_plain_parallel 8", lambda: import_plain_parallel(8)),
    ("import_prepared",         import_prepared),
    ("import_select",           import_select),
)

perf = {}
n = 5
for i in range(n):
    for t in tests:
        if t[0] not in perf:
            perf[t[0]] = []
        perf[t[0]].append(t[1]())

for k in sorted(perf.keys(), key=lambda k: perf[k][(0 + n - 1) // 2]):
    a = sorted(perf[k])
    print(k, a)
