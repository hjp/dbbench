#!/usr/bin/python3

"""
Compare 3 ways of importing of a largish csv file into a pg table:
1) Plain inserts (with varying number of processes)
2) Prepare/Execute
3) Copy (with varying number of processes)

Additionally, "import" the data from a different table in the same
database.

We perform each variant 5 times and report the median.
"""

import csv
import os
import psycopg
import sys
import time
import multiprocessing as mp

db = psycopg.connect("")
csr = db.cursor()

def create_table():
    with open(sys.argv[1], encoding="utf-8-sig") as f:
        rdr = csv.reader(f,delimiter=",")
        header = next(rdr)
        try:
            create_table.count += 1
        except:
            create_table.count = 1
        table_name = "import_pg_comparison_%d_%d" % (os.getpid(), create_table.count)
        q_drop = "drop table if exists " + table_name
        csr.execute(q_drop);

        q_create = (
            "create table " + table_name + "(" +
            ", ".join('"%s" varchar' % x for x in header) +
            ")"
        )
        # print(q_create)
        csr.execute(q_create);
    return table_name

def import_plain():
    table_name = create_table()

    with open(sys.argv[1], encoding="utf-8-sig") as f:
        rdr = csv.reader(f,delimiter=",")
        header = next(rdr)
        q_insert = (
            "insert into " + table_name +
            " values(" + ", ".join(["%s"] * len(header)) + ")"
        )
        # print(q_insert)
        db.commit()
        t0 = time.monotonic()
        for line in rdr:
            csr.execute(q_insert, line)

    db.commit()
    t1 = time.monotonic()
    dt = t1 - t0
    csr.execute("select count(*) from " + table_name)
    r = csr.fetchone()[0]
    print("imported %d rows in %f seconds (%f rows per second)" % (r, dt, r / dt))
    q_drop = "drop table if exists " + table_name
    csr.execute(q_drop);
    return r / dt

def import_copy():
    print("import_copy", end=": ", flush=True)
    table_name = create_table()

    db.commit()
    t0 = time.monotonic()
    with open(sys.argv[1], encoding="utf-8-sig") as f:
        rdr = csv.reader(f,delimiter=",")
        header = next(rdr)
        q = "copy " + table_name + "(" + ", ".join('"%s"' % x for x in header) + ") from stdin"
        with csr.copy(q) as copy:
            for line in rdr:
                copy.write_row(line)
    db.commit()
    t1 = time.monotonic()
    dt = t1 - t0
    csr.execute("select count(*) from " + table_name)
    r = csr.fetchone()[0]
    print("imported %d rows in %f seconds (%f rows per second)" % (r, dt, r / dt))
    q_drop = "drop table if exists " + table_name
    csr.execute(q_drop);
    return r / dt

def import_select():
    """
    "Import" data from another table in the database.
    """
    table_name_1 = create_table()
    table_name_2 = create_table()

    db.commit()
    # Initialize table 1 with copy.
    # This isn't part of the actual test.
    with open(sys.argv[1], encoding="utf-8-sig") as f:
        rdr = csv.reader(f,delimiter=",")
        header = next(rdr)
        q = "copy " + table_name_1 + "(" + ", ".join('"%s"' % x for x in header) + ") from stdin"
        with csr.copy(q) as copy:
            for line in rdr:
                copy.write_row(line)
    db.commit()

    t0 = time.monotonic()
    csr.execute("insert into %s select * from %s" % (table_name_2, table_name_1))
    db.commit()
    t1 = time.monotonic()
    dt = t1 - t0
    csr.execute("select count(*) from " + table_name_2)
    r = csr.fetchone()[0]
    print("imported %d rows in %f seconds (%f rows per second)" % (r, dt, r / dt))
    q_drop = "drop table if exists " + table_name_1
    csr.execute(q_drop);
    q_drop = "drop table if exists " + table_name_2
    csr.execute(q_drop);
    db.commit()
    return r / dt

def import_plain_parallel_worker(i, n, file_name, table_name):
    db = psycopg.connect("")
    csr = db.cursor()

    with open(file_name, encoding="utf-8-sig") as f:
        rdr = csv.reader(f,delimiter=",")
        header = next(rdr)
        q_insert = (
            "insert into " + table_name +
            " values(" + ", ".join(["%s"] * len(header)) + ")"
        )
        #print(q_insert)
        c = 0
        for line in rdr:
            if c % n == i:
                csr.execute(q_insert, line)
            c += 1

    db.commit()

def import_plain_parallel(n):
    print("import_plain_parallel", n, end=": ", flush=True)
    table_name = create_table()
    db.commit()

    t0 = time.monotonic()
    kids = []
    for i in range(n):
        p = mp.Process(
                target=import_plain_parallel_worker,
                args=(i, n, sys.argv[1], table_name)
               )
        p.start()
        kids.append(p)
    for p in kids:
        p.join()

    t1 = time.monotonic()
    dt = t1 - t0
    csr.execute("select count(*) from " + table_name)
    r = csr.fetchone()[0]
    print("imported %d rows in %f seconds (%f rows per second)" % (r, dt, r / dt))
    q_drop = "drop table if exists " + table_name
    csr.execute(q_drop);
    return r / dt

def import_copy_parallel_worker(i, n, file_name, table_name):
    db = psycopg.connect("")
    csr = db.cursor()

    with open(file_name, encoding="utf-8-sig") as f:
        rdr = csv.reader(f,delimiter=",")
        header = next(rdr)
        q = "copy " + table_name + "(" + ", ".join('"%s"' % x for x in header) + ") from stdin"
        with csr.copy(q) as copy:
            c = 0
            for line in rdr:
                if c % n == i:
                    copy.write_row(line)
                c += 1
    db.commit()

def import_copy_parallel(n):
    print("import_copy_parallel", n, end=": ", flush=True)
    table_name = create_table()
    db.commit()

    t0 = time.monotonic()
    kids = []
    for i in range(n):
        p = mp.Process(
                target=import_copy_parallel_worker,
                args=(i, n, sys.argv[1], table_name)
               )
        p.start()
        kids.append(p)
    for p in kids:
        p.join()

    t1 = time.monotonic()
    dt = t1 - t0
    csr.execute("select count(*) from " + table_name)
    r = csr.fetchone()[0]
    print("imported %d rows in %f seconds (%f rows per second)" % (r, dt, r / dt))
    q_drop = "drop table if exists " + table_name
    csr.execute(q_drop);
    return r / dt

def import_executemany():
    # Executemany is supposed to use pipeline mode implicitely, so it should be
    # quite a bit faster than plain, but not as fast as copy
    print("import_executemany", end=": ", flush=True)
    table_name = create_table()

    data = []
    with open(sys.argv[1], encoding="utf-8-sig") as f:
        rdr = csv.reader(f,delimiter=",")
        header = next(rdr)
        q_insert = (
            "insert into " + table_name +
            " values(" + ", ".join(["%s"] * len(header)) + ")"
        )
        #print(q_insert)
        db.commit()
        t0 = time.monotonic()
        for line in rdr:
            data.append(line)

    csr.executemany(q_insert, data)
    db.commit()
    t1 = time.monotonic()
    dt = t1 - t0
    csr.execute("select count(*) from " + table_name)
    r = csr.fetchone()[0]
    print("imported %d rows in %f seconds (%f rows per second)" % (r, dt, r / dt))
    q_drop = "drop table if exists " + table_name
    csr.execute(q_drop);
    return r / dt

def import_pipeline():
    # Explicitely use pipeline mode.
    print("import_pipeline", end=": ", flush=True)
    table_name = create_table()

    with open(sys.argv[1], encoding="utf-8-sig") as f:
        rdr = csv.reader(f,delimiter=",")
        header = next(rdr)
        q_insert = (
            "insert into " + table_name +
            " values(" + ", ".join(["%s"] * len(header)) + ")"
        )
        #print(q_insert)
        db.commit()
        t0 = time.monotonic()
        with db.pipeline():
            csr = db.cursor()
            for line in rdr:
                csr.execute(q_insert, line)

    db.commit()
    t1 = time.monotonic()
    dt = t1 - t0
    csr.execute("select count(*) from " + table_name)
    r = csr.fetchone()[0]
    print("imported %d rows in %f seconds (%f rows per second)" % (r, dt, r / dt))
    q_drop = "drop table if exists " + table_name
    csr.execute(q_drop);
    return r / dt


tests = (
    ("import_copy",             import_copy),
    ("import_copy_parallel 2",  lambda: import_copy_parallel(2)),
    ("import_copy_parallel 4",  lambda: import_copy_parallel(4)),
    ("import_copy_parallel 8",  lambda: import_copy_parallel(8)),
    ("import_plain",            import_plain),
    ("import_plain_parallel 2", lambda: import_plain_parallel(2)),
    ("import_plain_parallel 4", lambda: import_plain_parallel(4)),
    ("import_plain_parallel 8", lambda: import_plain_parallel(8)),
    ("import_select",           import_select),
    ("import_executemany",      import_executemany),
    ("import_pipeline",         import_pipeline),
)

perf = {}
n = 5
for i in range(n):
    print(f"Round {i}")
    for t in tests:
        if t[0] not in perf:
            perf[t[0]] = []
        perf[t[0]].append(t[1]())

for k in sorted(perf.keys(), key=lambda k: perf[k][(0 + n - 1) // 2]):
    a = sorted(perf[k])
    print(k, a)
